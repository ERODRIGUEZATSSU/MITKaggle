{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.12"},"papermill":{"default_parameters":{},"duration":961.084151,"end_time":"2023-02-15T03:36:18.894548","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2023-02-15T03:20:17.810397","version":"2.3.4"}},"nbformat_minor":5,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":0.036092,"end_time":"2023-02-15T03:20:28.209393","exception":false,"start_time":"2023-02-15T03:20:28.173301","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T20:09:54.477636Z","iopub.execute_input":"2023-04-01T20:09:54.477996Z","iopub.status.idle":"2023-04-01T20:09:54.500051Z","shell.execute_reply.started":"2023-04-01T20:09:54.477963Z","shell.execute_reply":"2023-04-01T20:09:54.499033Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/entire-data/all_data.csv\n/kaggle/input/breakthrough-tech-ai-studio-challenge/sample_submission.csv\n/kaggle/input/breakthrough-tech-ai-studio-challenge/movies_keywords.csv\n/kaggle/input/breakthrough-tech-ai-studio-challenge/movies_metadata.csv\n/kaggle/input/breakthrough-tech-ai-studio-challenge/train.csv\n/kaggle/input/breakthrough-tech-ai-studio-challenge/test.csv\n/kaggle/input/mergedata/merge_data.ipynb\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Train Dataset","metadata":{"papermill":{"duration":0.010759,"end_time":"2023-02-15T03:20:28.231441","exception":false,"start_time":"2023-02-15T03:20:28.220682","status":"completed"},"tags":[]}},{"cell_type":"code","source":"train = pd.read_csv(\"/kaggle/input/breakthrough-tech-ai-studio-challenge/train.csv\")\ntrain.head()","metadata":{"papermill":{"duration":0.110026,"end_time":"2023-02-15T03:20:28.351733","exception":false,"start_time":"2023-02-15T03:20:28.241707","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T19:21:55.857428Z","iopub.execute_input":"2023-04-01T19:21:55.857713Z","iopub.status.idle":"2023-04-01T19:21:55.902711Z","shell.execute_reply.started":"2023-04-01T19:21:55.857687Z","shell.execute_reply":"2023-04-01T19:21:55.901267Z"},"trusted":true},"execution_count":9,"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"  userId_movieId  rating\n0        10_1358     0.4\n1       237_1544     0.7\n2         54_373     1.0\n3        11_2053     0.8\n4       183_2524     0.6","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>userId_movieId</th>\n      <th>rating</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>10_1358</td>\n      <td>0.4</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>237_1544</td>\n      <td>0.7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>54_373</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>11_2053</td>\n      <td>0.8</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>183_2524</td>\n      <td>0.6</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#split \"userId_movieId\" column to two 'userId', 'movieId'\ntrain[['userId', 'movieId']] = train[\"userId_movieId\"].apply(lambda x: pd.Series(str(x).split(\"_\")))\ntrain = train.drop('userId_movieId', axis=1)\ntrain.head()","metadata":{"papermill":{"duration":14.316529,"end_time":"2023-02-15T03:20:42.712450","exception":false,"start_time":"2023-02-15T03:20:28.395921","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T19:21:55.905149Z","iopub.execute_input":"2023-04-01T19:21:55.905661Z","iopub.status.idle":"2023-04-01T19:22:10.575323Z","shell.execute_reply.started":"2023-04-01T19:21:55.905621Z","shell.execute_reply":"2023-04-01T19:22:10.573854Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"   rating userId movieId\n0     0.4     10    1358\n1     0.7    237    1544\n2     1.0     54     373\n3     0.8     11    2053\n4     0.6    183    2524","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>rating</th>\n      <th>userId</th>\n      <th>movieId</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.4</td>\n      <td>10</td>\n      <td>1358</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.7</td>\n      <td>237</td>\n      <td>1544</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1.0</td>\n      <td>54</td>\n      <td>373</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.8</td>\n      <td>11</td>\n      <td>2053</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.6</td>\n      <td>183</td>\n      <td>2524</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"#change the datatype of the id columns from object to integers\ntrain = train.astype({\"userId\":\"int\",\"movieId\":\"int\"})\nprint(train)","metadata":{"papermill":{"duration":0.043559,"end_time":"2023-02-15T03:20:42.828240","exception":false,"start_time":"2023-02-15T03:20:42.784681","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T19:22:10.577644Z","iopub.execute_input":"2023-04-01T19:22:10.578131Z","iopub.status.idle":"2023-04-01T19:22:10.613518Z","shell.execute_reply.started":"2023-04-01T19:22:10.578087Z","shell.execute_reply":"2023-04-01T19:22:10.612188Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"       rating  userId  movieId\n0         0.4      10     1358\n1         0.7     237     1544\n2         1.0      54      373\n3         0.8      11     2053\n4         0.6     183     2524\n...       ...     ...      ...\n69997     0.7     308      356\n69998     0.6     500      223\n69999     1.0     617     2722\n70000     0.8     305    45987\n70001     0.8     305     2130\n\n[70002 rows x 3 columns]\n","output_type":"stream"}]},{"cell_type":"code","source":"train.info()","metadata":{"papermill":{"duration":0.030451,"end_time":"2023-02-15T03:20:42.869539","exception":false,"start_time":"2023-02-15T03:20:42.839088","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T19:22:10.618149Z","iopub.execute_input":"2023-04-01T19:22:10.618532Z","iopub.status.idle":"2023-04-01T19:22:10.632860Z","shell.execute_reply.started":"2023-04-01T19:22:10.618499Z","shell.execute_reply":"2023-04-01T19:22:10.631513Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 70002 entries, 0 to 70001\nData columns (total 3 columns):\n #   Column   Non-Null Count  Dtype  \n---  ------   --------------  -----  \n 0   rating   70002 non-null  float64\n 1   userId   70002 non-null  int64  \n 2   movieId  70002 non-null  int64  \ndtypes: float64(1), int64(2)\nmemory usage: 1.6 MB\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Principal Component Analysis - PCA ","metadata":{"papermill":{"duration":0.010856,"end_time":"2023-02-15T03:20:42.891408","exception":false,"start_time":"2023-02-15T03:20:42.880552","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"Principal Component Analysis is a powerful technique for reducing the dimensionality of complex datasets by finding the directions of maximum variance in the data and projecting the data onto these directions. It is widely used in data analysis and machine learning for simplifying complex datasets and improving the accuracy and efficiency of machine learning models. PCA is used for dimensionality reduction because it can help simplify complex datasets and remove noise and redundancy. By reducing the number of dimensions in the data, we can reduce the computational complexity of machine learning algorithms, speed up the training process, and improve the accuracy of the model.","metadata":{"papermill":{"duration":0.01065,"end_time":"2023-02-15T03:20:42.912924","exception":false,"start_time":"2023-02-15T03:20:42.902274","status":"completed"},"tags":[]}},{"cell_type":"markdown","source":"**MinMaxScaler**","metadata":{}},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/final-movie-metadata/final_movie_metadata.csv\")\ndata.head()","metadata":{"execution":{"iopub.status.busy":"2023-04-01T20:25:32.840114Z","iopub.execute_input":"2023-04-01T20:25:32.841039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# first find the number of components using MinMaxScaler\nfrom sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndata_rescaled = scaler.fit_transform(train)","metadata":{"papermill":{"duration":6.110844,"end_time":"2023-02-15T03:20:49.080089","exception":false,"start_time":"2023-02-15T03:20:42.969245","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2023-04-01T19:22:10.634371Z","iopub.execute_input":"2023-04-01T19:22:10.634871Z","iopub.status.idle":"2023-04-01T19:22:10.648129Z","shell.execute_reply.started":"2023-04-01T19:22:10.634806Z","shell.execute_reply":"2023-04-01T19:22:10.647057Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# 95% or 99% of variance \nfrom sklearn.decomposition import PCA\npca = PCA(n_components = 0.95)\npca.fit(data_rescaled)\ndata_reduced = pca.transform(data_rescaled)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:22:10.650184Z","iopub.execute_input":"2023-04-01T19:22:10.650560Z","iopub.status.idle":"2023-04-01T19:22:10.805082Z","shell.execute_reply.started":"2023-04-01T19:22:10.650522Z","shell.execute_reply":"2023-04-01T19:22:10.803510Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"print(data_reduced)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:22:10.806575Z","iopub.execute_input":"2023-04-01T19:22:10.807286Z","iopub.status.idle":"2023-04-01T19:22:10.816048Z","shell.execute_reply.started":"2023-04-01T19:22:10.807244Z","shell.execute_reply":"2023-04-01T19:22:10.814486Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"[[ 0.49293871  0.35349069 -0.05155652]\n [ 0.16259121  0.01183365 -0.05810513]\n [ 0.44352109 -0.31449153 -0.03926992]\n ...\n [-0.3960323  -0.33545155 -0.07117036]\n [ 0.04879994 -0.09323984  0.21048714]\n [ 0.06371889 -0.10165986 -0.05646931]]\n","output_type":"stream"}]},{"cell_type":"code","source":"target = train['rating']\nprint(target)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:23:49.856492Z","iopub.execute_input":"2023-04-01T19:23:49.856876Z","iopub.status.idle":"2023-04-01T19:23:49.865402Z","shell.execute_reply.started":"2023-04-01T19:23:49.856836Z","shell.execute_reply":"2023-04-01T19:23:49.864123Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"0        0.4\n1        0.7\n2        1.0\n3        0.8\n4        0.6\n        ... \n69997    0.7\n69998    0.6\n69999    1.0\n70000    0.8\n70001    0.8\nName: rating, Length: 70002, dtype: float64\n","output_type":"stream"}]},{"cell_type":"markdown","source":"****RandomForestRegressor Model****","metadata":{}},{"cell_type":"markdown","source":"Step 1: Load the data and split into training and testing sets","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ndata = data_rescaled\nX_train, X_test, y_train, y_test = train_test_split(data.data, target, test_size=0.3, random_state=42)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:24:22.766154Z","iopub.execute_input":"2023-04-01T19:24:22.767112Z","iopub.status.idle":"2023-04-01T19:24:22.790592Z","shell.execute_reply.started":"2023-04-01T19:24:22.767073Z","shell.execute_reply":"2023-04-01T19:24:22.789026Z"},"trusted":true},"execution_count":20,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_23/1457973353.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata_rescaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2443\u001b[0m     return list(\n\u001b[1;32m   2444\u001b[0m         chain.from_iterable(\n\u001b[0;32m-> 2445\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2446\u001b[0m         )\n\u001b[1;32m   2447\u001b[0m     )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   2443\u001b[0m     return list(\n\u001b[1;32m   2444\u001b[0m         chain.from_iterable(\n\u001b[0;32m-> 2445\u001b[0;31m             \u001b[0;34m(\u001b[0m\u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2446\u001b[0m         )\n\u001b[1;32m   2447\u001b[0m     )\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[0;34m(X, indices, axis)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"shape\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_list_indexing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py\u001b[0m in \u001b[0;36m_array_indexing\u001b[0;34m(array, key, key_dtype, axis)\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"],"ename":"TypeError","evalue":"only integer scalar arrays can be converted to a scalar index","output_type":"error"}]},{"cell_type":"markdown","source":"Step 2. Perform PCA on training data","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=5)\nX_train_pca = pca.fit_transform(X_train)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:22:10.872597Z","iopub.status.idle":"2023-04-01T19:22:10.876597Z","shell.execute_reply.started":"2023-04-01T19:22:10.876187Z","shell.execute_reply":"2023-04-01T19:22:10.876241Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 3: Train the Random Forest Regressor model on the transformed training data","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(n_estimators=100, random_state=42)\nrf.fit(X_train_pca, y_train)","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:22:10.881617Z","iopub.status.idle":"2023-04-01T19:22:10.882902Z","shell.execute_reply.started":"2023-04-01T19:22:10.882501Z","shell.execute_reply":"2023-04-01T19:22:10.882540Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Step 4: Transform the testing data using the same PCA transformation and evaluate the model","metadata":{}},{"cell_type":"code","source":"X_test_pca = pca.transform(X_test)\nscore = rf.score(X_test_pca, y_test)\nprint(\"R^2 Score: {:.2f}\".format(score))","metadata":{"execution":{"iopub.status.busy":"2023-04-01T19:22:10.891438Z","iopub.status.idle":"2023-04-01T19:22:10.892827Z","shell.execute_reply.started":"2023-04-01T19:22:10.892410Z","shell.execute_reply":"2023-04-01T19:22:10.892454Z"},"trusted":true},"execution_count":null,"outputs":[]}]}